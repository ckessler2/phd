
import tensorflow as tf
import numpy as np

class TrainingHistory:
    def __init__(self):
        self.history = {
            "loss": [],
            "val_loss": []
        }

    def add_entry(self, loss, val_loss):
        # Ensure data is in the right format
        loss = np.float32(loss)
        val_loss = np.float32(val_loss)
        
        # Append data to respective lists
        self.history["loss"].append(loss)
        self.history["val_loss"].append(val_loss)

class AdversarialTrainer:
    """
    A class for training models with adversarial examples to improve robustness
    against adversarial attacks within an epsilon-ball (L-infinity norm).
    """

    def __init__(self, model, epsilon=0.1):
        """
        Initialises the AdversarialTrainer with a model and a perturbation factor (epsilon).

        Args:
            model (tf.keras.Model): The model to be trained and defended against adversarial attacks.
            epsilon (float): The maximum allowable perturbation for generating adversarial examples.
        """
        self.model = model
        self.epsilon = epsilon

    def generate_adversarial_example(self, x, y):
        """
        Generates an adversarial example for a given input using the Fast Gradient Sign Method (FGSM).

        Args:
            x (tf.Tensor): The input data (features) for which the adversarial example is generated.
            y (tf.Tensor): The true labels corresponding to the input data.

        Returns:
            tf.Tensor: The adversarial example generated by perturbing the input data within the epsilon-ball.
        """
        with tf.GradientTape() as tape:
            tape.watch(x)
            prediction = self.model(x, training=False)
            # Use the MeanSquaredError instance
            loss_fn = tf.keras.losses.MeanSquaredError()
            loss = loss_fn(y, prediction)

        # Calculate the gradient of the loss with respect to the input
        gradient = tape.gradient(loss, x)

        # Extract the sign of the gradient
        signed_grad = tf.sign(gradient)

        # Create the adversarial example by adding a small perturbation within the epsilon-ball
        adversarial_x = x + self.epsilon * signed_grad
        return tf.clip_by_value(
            adversarial_x, 0, 1
        )  # Ensure values stay within [0, 1] bounds

    def train_with_adversarial_examples(self, train_dataset, epochs=10, callbacks=None):
        """
        Trains the model on both normal and adversarial examples to enforce robustness within an epsilon-ball.

        Args:
            train_dataset (tf.data.Dataset): The training dataset providing input-output pairs (x, y).
            epochs (int): The number of epochs for training.
            callbacks (list): A list of callbacks to be invoked during the training process.
                              Defaults to None.

        Returns:
            tf.keras.Model: The trained model after adversarial training.

        Raises:
            ValueError: If the training dataset is not properly formatted.
        """
        if callbacks is None:
            callbacks = []
            
        history2 = TrainingHistory()

        for epoch in range(epochs):
            for callback in callbacks:
                callback.on_epoch_begin(epoch)

            for x_batch, y_batch in train_dataset:
                # Generate adversarial examples for the batch within the epsilon-ball
                adversarial_x_batch = self.generate_adversarial_example(
                    x_batch, y_batch
                )

                # Concatenate original and adversarial examples
                combined_x = tf.concat([x_batch, adversarial_x_batch], axis=0)
                combined_y = tf.concat([y_batch, y_batch], axis=0)

                # Train on combined data
                history = self.model.train_on_batch(combined_x, combined_y)
                loss = np.float32(history[0])
                val_loss = np.float32(history[1])
                
                # history2["history"]["loss"].append(np.float32(history[0]))
                # history2["history"]["val_loss"].append(val_loss)
                history2.add_entry(loss, val_loss)

            for callback in callbacks:
                callback.on_epoch_end(epoch)

        print("Adversarial training with epsilon-ball robustness completed.")
        return self.model, history2

